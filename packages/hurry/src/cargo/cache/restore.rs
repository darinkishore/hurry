use std::{
    collections::{HashMap, HashSet},
    sync::Arc,
    time::{Duration, SystemTime},
};

use color_eyre::{
    Result,
    eyre::{Context as _, OptionExt as _, bail},
};
use dashmap::{DashMap, DashSet};
use derive_more::Debug;
use futures::{StreamExt, future::BoxFuture};
use serde::{Deserialize, Serialize};
use tokio::task::JoinSet;
use tracing::{Instrument, debug, info, instrument, trace, warn};

use crate::{
    cargo::{self, Fingerprint, QualifiedPath, UnitHash, UnitPlan, Workspace, host_glibc_version},
    cas::CourierCas,
    fs,
    path::JoinWith as _,
    progress::TransferBar,
};
use clients::{
    Courier,
    courier::v1::{Key, SavedUnit, cache::CargoRestoreRequest, cache::CargoRestoreResponse},
};

/// Tracks items that were restored from the cache.
#[derive(Debug, Clone, Default, Serialize, Deserialize)]
pub struct Restored {
    /// Stores the unit hashes of restored units.
    pub units: DashSet<UnitHash>,
    pub files: DashSet<Key>,
}

#[derive(Debug)]
struct FileRestoreKey {
    unit_hash: UnitHash,
    key: Key,
    #[allow(
        clippy::type_complexity,
        reason = "it's a closure that returns a future of Result<()>"
    )]
    #[debug(skip)]
    write: Box<dyn FnOnce(&Vec<u8>) -> BoxFuture<'static, Result<()>> + Send + Sync>,
}

/// Tracks restore progress. It does this by tracking which units have been
/// queued for restore, and which of their files _remain_ to be restored. After
/// each file is restored, we remove it from its unit's set of pending files.
/// When the set of pending files for a unit is empty, we know that the unit has
/// been fully restored, because we added all of the unit's files to its pending
/// set before restoring any files.
#[derive(Debug, Clone, Default)]
struct RestoreProgress {
    units: Arc<DashMap<UnitHash, DashSet<Key>>>,
}

#[instrument(skip(units, progress))]
pub async fn restore_units(
    courier: &Courier,
    cas: &CourierCas,
    ws: &Workspace,
    units: &Vec<UnitPlan>,
    progress: &TransferBar,
) -> Result<Restored> {
    trace!(?units, "units");

    let restored = Restored::default();

    // Check which units are already on disk, and don't attempt to restore them.
    // Note that this does not attempt to check actual _freshness_, since that
    // logic is quite complicated[^1] and involves synthesizing a complete
    // fingerprint for comparison. Instead, we merely check for _presence_,
    // since it would be pretty unlikely for a unit's fingerprint to exist
    // already but for the unit to be dirty. In either case, Cargo's own
    // freshness detection still runs when we shell out to it, so
    // present-but-not-fresh units are still compiled.
    //
    // [^1]: https://github.com/attunehq/cargo/blob/10fcf1b64e201d1754b50be76a7d2db269d81408/src/cargo/core/compiler/fingerprint/mod.rs#L994
    let mut units_to_skip: HashSet<UnitHash> = HashSet::new();
    for unit in units {
        let info = unit.info();
        // TODO: We should really just check the existence of the entire unit's
        // expected outputs, since sometimes a partial restore interrupted by ^C
        // will pass this check but fail to build because the small fingerprints
        // restored but the large libraries did not.
        if fs::exists(
            &ws.unit_profile_dir(info)
                .join(unit.fingerprint_json_file()?),
        )
        .await
        {
            // TODO: We actually don't want to always skip uploading the unit
            // because we might not have the unit uploaded remotely. What we
            // really want to do is:
            //
            // 1. Calculate the unit plan.
            // 2. Call the API for _all_ units in the plan, so we know which are and are not
            //    stored but not present.
            // 3. Iterate through all units in the unit plan, restoring it only if it is not
            //    present, and marking it for upload if it is not stored.
            units_to_skip.insert(info.unit_hash.clone());
            debug!(
                unit_hash = ?info.unit_hash,
                pkg_name = %info.package_name,
                "skipping unit: already fresh locally"
            );
            debug!(?unit, "marking unit as restored after skipping");
            restored.units.insert(info.unit_hash.clone());
        }
    }

    // If this build is against glibc, we need to know the glibc version so we
    // don't restore objects that link to missing symbols.
    let host_glibc_symbol_version = host_glibc_version()?;
    debug!(
        ?host_glibc_symbol_version,
        "restore starting with host glibc"
    );

    // Load unit information from remote cache. Note that this does NOT download
    // the actual files, which are loaded as CAS keys.
    //
    // We request ALL units (including skipped ones) because we need their
    // fingerprint data to populate `dep_fingerprints`. When a unit is skipped
    // (already on disk), we still need to know its cached fingerprint hash so
    // that dependent units can rewrite their fingerprint references correctly.
    //
    // TODO(#297): Ideally, we would load the fingerprints from units that exist
    // on disk from the disk, which would avoid making the network request
    // larger. This would require reading the fingerprint JSON files for skipped
    // units and merging them with the network response.
    let requested_count = units.len();
    let bulk_req = CargoRestoreRequest::new(
        units.iter().map(|unit| unit.info().unit_hash.clone()),
        host_glibc_symbol_version,
    );
    info!(requested_count, "requesting units from cache");
    let mut saved_units = courier.cargo_cache_restore(bulk_req).await?;
    info!(
        requested_count,
        returned_count = saved_units.len(),
        "cache restore response"
    );

    // Filter units with incomplete dependency chains.
    // Units whose transitive dependencies are not all available (either in
    // cache or on disk) will be skipped, because:
    // 1. Fingerprint rewriting would fail due to missing dep hash mappings
    // 2. Cargo would rebuild them anyway due to transitive staleness
    // TODO: Consider fusing this filtering into the main restore loop (along
    // with units_to_skip) to make the elementwise nature of the computation
    // clearer and improve locality.
    let (units_with_incomplete_deps, incomplete_deps_count) =
        filter_units_with_incomplete_deps(units, &saved_units, &units_to_skip);
    if incomplete_deps_count > 0 {
        warn!(
            incomplete_deps_count,
            "filtered units with incomplete dependency chains"
        );
    }

    // Track restore progress.
    let restore_progress = RestoreProgress::default();

    // Spawn concurrent workers for doing parallel downloads.
    let (tx, mut workers) = {
        let worker_count = num_cpus::get();
        // We use an unbounded channel here because if we use a bounded channel,
        // errors in the client then (incorrectly) get clobbered by the error
        // caused by sending to a closed channel. We already buffer the entire
        // set of work items we want to send, so using an unbounded channel for
        // it doesn't cause additional memory pressure- we just move our
        // buffered set of work items into the channel all at once instead of as
        // they're being worked on.
        let (tx, rx) = flume::unbounded::<FileRestoreKey>();
        let mut workers = JoinSet::new();
        for worker_id in 0..worker_count {
            let rx = rx.clone();
            let cas = cas.clone();
            let progress = progress.clone();
            let restored = restored.clone();
            let restore_progress = restore_progress.clone();
            let span = tracing::info_span!("restore_worker", worker_id);
            workers.spawn(
                restore_worker(rx, cas, progress, restored, restore_progress).instrument(span),
            );
        }
        // Dropping the `rx` causes it to close, so we cannot drop it until all
        // workers have finished receiving files.
        (tx, workers)
    };

    let mut dep_fingerprints = HashMap::new();
    let mut files_to_restore = Vec::<FileRestoreKey>::new();
    // We anchor the starting mtime to the Unix epoch to avoid dirtying
    // first-party package builds in multi-package workspaces when we restore
    // dependencies.
    //
    // For example, consider the case where A depends on B depends on C, where A
    // and B are first-party packages and C is a third-party dependency. Imagine
    // first compiling B, and then compiling A. The sequence of events would
    // then be:
    //
    // 0. Restore A from cache.
    // 1. Compile B.
    // 2. Restore A from cache (which is a no-op transfer but still sets mtimes).
    // 3. Compile A.
    //
    // The problem is that A now has mtime 2 and B has mtime 1, so B is marked
    // dirty _even though it isn't_. In order to avoid this problem, we fix the
    // mtime of restored dependencies to 0, because hopefully none of our users
    // have a time machine capable of travelling back to 1970. This way,
    // dependency mtimes will _always_ be older than first-party package mtimes,
    // even after multiple restores.
    //
    // This works because third-party packages can never depend on local
    // first-party workspace packages. When we begin caching first-party
    // workspace packages as well, all of this logic needs to change (either by
    // setting all mtimes or by building some sort of constrained-graph mtime
    // solver).
    let starting_mtime = SystemTime::UNIX_EPOCH;
    // Shared references to clone once here instead of cloning once per unit.
    let ws = Arc::new(ws.clone());

    for (i, unit) in units.iter().enumerate() {
        debug!(?unit, "queuing unit restore");
        let unit_hash = &unit.info().unit_hash;

        // Calculate the mtime for files to be restored. All output file mtimes
        // for a unit U must be after those of U's dependencies (i.e. all of U's
        // mtimes must be before its dependents). To satisfy this property
        // easily, we set the mtime of all files in U to be the same, and
        // increment this mtime for every unit we see (since units are in
        // dependency order).
        //
        // We use a 1s increment here so that mtimes are still correctly set on
        // filesystems with low timestamp precision. For reference, see Cargo's
        // timestamp comparison logic.[^1]
        //
        // [^1]: https://github.com/rust-lang/cargo/blob/c24e1064277fe51ab72011e2612e556ac56addf7/src/cargo/core/compiler/fingerprint/mod.rs#L1229-L1235
        let mtime = starting_mtime + Duration::from_secs(i as u64);

        if units_with_incomplete_deps.contains(unit_hash) {
            progress.dec_length(1);
            continue;
        }

        // Load the saved file info from the response.
        let Some(saved) = saved_units.take(&unit_hash.into()) else {
            // Units may be missing from the cache response for various reasons:
            // - The unit was never uploaded (cache miss)
            // - The unit was evicted from the cache
            // - The unit was filtered out (e.g., glibc version incompatibility)
            // This is normal cache behavior - the unit will just be rebuilt.
            debug!(
                ?unit_hash,
                unit_type = %unit_type_name(unit),
                pkg_name = %unit.info().package_name,
                "unit missing from cache response"
            );

            // Even when skipped, unit mtimes must be updated to maintain the
            // invariant that dependencies always have older mtimes than their
            // dependents. Otherwise, units that are skipped may have mtimes
            // that are out of sync with units that are restored.
            if units_to_skip.contains(unit_hash)
                && let Err(err) = unit.touch(&ws, starting_mtime).await
            {
                warn!(?unit_hash, ?err, "could not set mtime for skipped unit");
            }
            progress.dec_length(1);
            continue;
        };

        // Parse the cached fingerprint from the saved unit. This is needed for
        // both skipped units (to record the mapping) and restored units (to
        // rewrite dependencies).
        let cached_fingerprint = saved.fingerprint().as_str();
        let cached_fingerprint = serde_json::from_str::<Fingerprint>(cached_fingerprint)?;

        // Handle skipped units that have been uploaded to cache.
        //
        // Skipped fingerprints are probably already correct on disk, because
        // the only things that can change about fingerprints are their path
        // field (the fingerprint exists so it was probably built locally so the
        // path is probably right) and deps field (the unit was probably built
        // locally and so probably expects its deps to have the right path).
        //
        // To keep them fresh, we just need to record the mapping from the
        // cached fingerprint (which is the fingerprint for this unit that all
        // the other units are expecting) to the local fingerprint (which is
        // functionally the same as the rewritten fingerprint) and touch the
        // unit mtime to maintain ordering invariants.
        if units_to_skip.contains(unit_hash) {
            // Read the local fingerprint from disk and record the mapping from
            // cached hash to local fingerprint. This allows dependent units to
            // properly rewrite their fingerprint references.
            let profile = ws.unit_profile_dir(unit.info());
            let cached_hash = cached_fingerprint.hash_u64();

            let file = unit.fingerprint_json_file()?;
            let file = profile.join(&file);
            let json = fs::must_read_buffered_utf8(&file).await?;
            // TODO: Maybe assert that this has the same value as a rewritten
            // fingerprint?
            let local = serde_json::from_str::<Fingerprint>(&json)?;
            // local_hash is only ever used in the debug!() call but cannot be
            // inlined because hash_u64 calls Fingerprint::hash which has its
            // own debug!() call and event generation cannot be nested[^1].
            //
            // [^1]: https://github.com/tokio-rs/tracing/issues/2448
            let local_hash = local.hash_u64();

            debug!(
                ?cached_hash,
                ?local_hash,
                "recorded fingerprint mapping for skipped unit"
            );

            dep_fingerprints.insert(cached_hash, local);

            if let Err(err) = unit.touch(&ws, mtime).await {
                warn!(?unit_hash, ?err, "could not set mtime for skipped unit");
            }
            progress.dec_length(1);
            continue;
        }

        // Handle restored unit fingerprints. These are written synchronously
        // during the loop because they need to be processed in dependency
        // order, since a unit's fingerprint depends on its dependencies'
        // fingerprints.
        //
        // TODO: Ideally, we would only write fingerprints _after_ all the files
        // for the unit are restored, to be maximally correct. This requires
        // plumbing some sort of work-dependency relationship between units and
        // restores.
        //
        // TODO: Maybe instead of this whole fingerprint-rewriting song and
        // dance, we should just fork and/or upstream relocatable fingerprints
        // into Cargo.
        let info = unit.info();
        let src_path = unit.src_path().map(|p| p.into());
        let rewritten_fingerprint = cached_fingerprint.rewrite(src_path, &mut dep_fingerprints)?;
        let fingerprint_hash = rewritten_fingerprint.fingerprint_hash();

        // Write the rewritten fingerprint.
        let profile_dir = ws.unit_profile_dir(info);
        fs::write(
            &profile_dir.join(&unit.fingerprint_hash_file()?),
            fingerprint_hash,
        )
        .await?;
        fs::write(
            &profile_dir.join(&unit.fingerprint_json_file()?),
            serde_json::to_vec(&rewritten_fingerprint)?,
        )
        .await?;

        // Mark the unit's restore as pending.
        restore_progress
            .units
            .insert(unit_hash.clone(), DashSet::new());

        // Queue all other files to be bulk-restored from CAS.
        match (saved, unit) {
            (
                SavedUnit::LibraryCrate(saved_library_files, _),
                UnitPlan::LibraryCrate(unit_plan),
            ) => {
                // Log detailed information about the library crate unit
                // to help debug cache restore issues (e.g., unit hash mismatches).
                trace!(
                    pkg_name = %unit_plan.info.package_name,
                    unit_hash = %unit_plan.info.unit_hash,
                    deps_dir = %unit_plan.info.deps_dir()?,
                    fingerprint_dir = %unit_plan.info.fingerprint_dir()?,
                    num_output_files = saved_library_files.output_files.len(),
                    "restoring library crate unit"
                );

                // Queue the output files.
                for file in saved_library_files.output_files {
                    let path: QualifiedPath = serde_json::from_str(file.path.as_str())?;
                    let path = path.reconstruct(&ws, &unit_plan.info).try_into()?;
                    let executable = file.executable;

                    restore_progress
                        .units
                        .get_mut(unit_hash)
                        .ok_or_eyre("unit hash restore progress not initialized")?
                        .insert(file.object_key.clone());
                    files_to_restore.push(FileRestoreKey {
                        unit_hash: unit_hash.clone(),
                        key: file.object_key.clone(),
                        write: Box::new(move |data| {
                            let data = data.clone();
                            Box::pin(async move {
                                fs::write(&path, data).await?;
                                fs::set_executable(&path, executable).await?;
                                fs::set_mtime(&path, mtime).await?;
                                Ok(())
                            })
                        }),
                    });
                }

                let profile_dir = ws.unit_profile_dir(&unit_plan.info);

                // Queue the dep-info file with reconstruction.
                let ws = ws.clone();
                let info = unit_plan.info.clone();
                let path = profile_dir.join(&unit_plan.dep_info_file()?);
                restore_progress
                    .units
                    .get_mut(unit_hash)
                    .ok_or_eyre("unit hash restore progress not initialized")?
                    .insert(saved_library_files.dep_info_file.clone());
                files_to_restore.push(FileRestoreKey {
                    unit_hash: unit_hash.clone(),
                    key: saved_library_files.dep_info_file.clone(),
                    write: Box::new(move |data| {
                        let data = data.clone();
                        Box::pin(async move {
                            let dep_info: cargo::DepInfo = serde_json::from_slice(&data)?;
                            let dep_info = dep_info.reconstruct(&ws, &info);
                            fs::write(&path, dep_info).await?;
                            fs::set_mtime(&path, mtime).await?;
                            Ok(())
                        })
                    }),
                });

                // Queue the encoded dep-info file (no transformation).
                let path = profile_dir.join(&unit_plan.encoded_dep_info_file()?);
                restore_progress
                    .units
                    .get_mut(unit_hash)
                    .ok_or_eyre("unit hash restore progress not initialized")?
                    .insert(saved_library_files.encoded_dep_info_file.clone());
                files_to_restore.push(FileRestoreKey {
                    unit_hash: unit_hash.clone(),
                    key: saved_library_files.encoded_dep_info_file.clone(),
                    write: Box::new(move |data| {
                        let data = data.clone();
                        Box::pin(async move {
                            fs::write(&path, data).await?;
                            fs::set_mtime(&path, mtime).await?;
                            Ok(())
                        })
                    }),
                });
            }
            (
                SavedUnit::BuildScriptCompilation(build_script_compiled_files, _),
                UnitPlan::BuildScriptCompilation(unit_plan),
            ) => {
                // Log detailed information about the build script compilation unit
                // to help debug cache restore issues.
                debug!(
                    pkg_name = %unit_plan.info.package_name,
                    unit_hash = %unit_plan.info.unit_hash,
                    fingerprint_dir = %unit_plan.info.fingerprint_dir()?,
                    "restoring build script compilation unit"
                );

                let profile_dir = ws.unit_profile_dir(&unit_plan.info);

                // Queue compiled program with hard link creation.
                let path = profile_dir.join(unit_plan.program_file()?);
                let linked_path = profile_dir.join(unit_plan.linked_program_file()?);
                restore_progress
                    .units
                    .get_mut(unit_hash)
                    .ok_or_eyre("unit hash restore progress not initialized")?
                    .insert(build_script_compiled_files.compiled_program.clone());
                files_to_restore.push(FileRestoreKey {
                    unit_hash: unit_hash.clone(),
                    key: build_script_compiled_files.compiled_program.clone(),
                    write: Box::new(move |data| {
                        let data = data.clone();
                        Box::pin(async move {
                            fs::write(&path, data).await?;
                            fs::set_executable(&path, true).await?;
                            fs::set_mtime(&path, mtime).await?;

                            fs::hard_link(&path, &linked_path).await?;
                            fs::set_mtime(&linked_path, mtime).await?;
                            Ok(())
                        })
                    }),
                });

                // Queue dep-info file with reconstruction.
                let ws = ws.clone();
                let info = unit_plan.info.clone();
                let path = profile_dir.join(&unit_plan.dep_info_file()?);
                restore_progress
                    .units
                    .get_mut(unit_hash)
                    .ok_or_eyre("unit hash restore progress not initialized")?
                    .insert(build_script_compiled_files.dep_info_file.clone());
                files_to_restore.push(FileRestoreKey {
                    unit_hash: unit_hash.clone(),
                    key: build_script_compiled_files.dep_info_file.clone(),
                    write: Box::new(move |data| {
                        let data = data.clone();
                        Box::pin(async move {
                            let dep_info: cargo::DepInfo = serde_json::from_slice(&data)?;
                            let dep_info = dep_info.reconstruct(&ws, &info);
                            fs::write(&path, dep_info).await?;
                            fs::set_mtime(&path, mtime).await?;
                            Ok(())
                        })
                    }),
                });

                // Queue encoded dep-info file (no transformation).
                let path = profile_dir.join(&unit_plan.encoded_dep_info_file()?);
                restore_progress
                    .units
                    .get_mut(unit_hash)
                    .ok_or_eyre("unit hash restore progress not initialized")?
                    .insert(build_script_compiled_files.encoded_dep_info_file.clone());
                files_to_restore.push(FileRestoreKey {
                    unit_hash: unit_hash.clone(),
                    key: build_script_compiled_files.encoded_dep_info_file.clone(),
                    write: Box::new(move |data| {
                        let data = data.clone();
                        Box::pin(async move {
                            fs::write(&path, data).await?;
                            fs::set_mtime(&path, mtime).await?;
                            Ok(())
                        })
                    }),
                });
            }
            (
                SavedUnit::BuildScriptExecution(build_script_output_files, _),
                UnitPlan::BuildScriptExecution(unit_plan),
            ) => {
                let profile_dir = ws.unit_profile_dir(&unit_plan.info);
                let out_dir = unit_plan.out_dir()?;
                let out_dir_absolute = profile_dir.join(&out_dir);

                // Log detailed information about the build script execution unit
                // to help debug cache restore issues (e.g., unit hash mismatches).
                debug!(
                    pkg_name = %unit_plan.info.package_name,
                    unit_hash = %unit_plan.info.unit_hash,
                    out_dir = %out_dir,
                    out_dir_absolute = %AsRef::<std::path::Path>::as_ref(&out_dir_absolute).display(),
                    build_dir = %unit_plan.info.build_dir()?,
                    fingerprint_dir = %unit_plan.info.fingerprint_dir()?,
                    num_out_dir_files = build_script_output_files.out_dir_files.len(),
                    "restoring build script execution unit"
                );

                // Create the OUT_DIR directory explicitly. This way, build
                // script execution units that have no OUT_DIR files will still
                // correctly have an empty OUT_DIR folder.
                fs::create_dir_all(&out_dir_absolute).await?;

                // Queue all OUT_DIR files with executable flag handling.
                for file in build_script_output_files.out_dir_files {
                    let path: QualifiedPath = serde_json::from_str(file.path.as_str())?;
                    let path = path.reconstruct(&ws, &unit_plan.info).try_into()?;
                    let executable = file.executable;

                    // Log each OUT_DIR file being restored (helpful for debugging
                    // native library issues like ring's libring_core_*.a).
                    debug!(
                        pkg_name = %unit_plan.info.package_name,
                        unit_hash = %unit_plan.info.unit_hash,
                        file_path = %AsRef::<std::path::Path>::as_ref(&path).display(),
                        executable = executable,
                        "restoring build script OUT_DIR file"
                    );

                    restore_progress
                        .units
                        .get_mut(unit_hash)
                        .ok_or_eyre("unit hash restore progress not initialized")?
                        .insert(file.object_key.clone());

                    files_to_restore.push(FileRestoreKey {
                        unit_hash: unit_hash.clone(),
                        key: file.object_key.clone(),
                        write: Box::new(move |data| {
                            let data = data.clone();
                            Box::pin(async move {
                                fs::write(&path, data).await?;
                                fs::set_executable(&path, executable).await?;
                                fs::set_mtime(&path, mtime).await?;
                                Ok(())
                            })
                        }),
                    });
                }

                // Queue stdout with BuildScriptOutput reconstruction.
                let ws = ws.clone();
                let info = unit_plan.info.clone();
                let path = profile_dir.join(&unit_plan.stdout_file()?);
                restore_progress
                    .units
                    .get_mut(unit_hash)
                    .ok_or_eyre("unit hash restore progress not initialized")?
                    .insert(build_script_output_files.stdout.clone());
                files_to_restore.push(FileRestoreKey {
                    unit_hash: unit_hash.clone(),
                    key: build_script_output_files.stdout.clone(),
                    write: Box::new(move |data| {
                        let data = data.clone();
                        Box::pin(async move {
                            let stdout: cargo::BuildScriptOutput = serde_json::from_slice(&data)?;
                            let stdout = stdout.reconstruct(&ws, &info);
                            fs::write(&path, stdout).await?;
                            fs::set_mtime(&path, mtime).await?;
                            Ok(())
                        })
                    }),
                });

                // Queue stderr (no transformation).
                let path = profile_dir.join(&unit_plan.stderr_file()?);
                restore_progress
                    .units
                    .get_mut(unit_hash)
                    .ok_or_eyre("unit hash restore progress not initialized")?
                    .insert(build_script_output_files.stderr.clone());
                files_to_restore.push(FileRestoreKey {
                    unit_hash: unit_hash.clone(),
                    key: build_script_output_files.stderr.clone(),
                    write: Box::new(move |data| {
                        let data = data.clone();
                        Box::pin(async move {
                            fs::write(&path, data).await?;
                            fs::set_mtime(&path, mtime).await?;
                            Ok(())
                        })
                    }),
                });

                // Generate root-output file (not from CAS - synthesized from unit_plan).
                // The root-output file must contain an absolute path because Cargo uses it
                // to rewrite paths in the build script output file. If we write a relative
                // path, Cargo's string replacement will match a substring of absolute paths
                // and cause path doubling (e.g.,
                // `/foo/target/release//foo/target/release/...`).
                //
                // See Cargo's `prev_build_output()` which reads this file
                // (custom_build.rs:1356-1361) and `BuildOutput::parse()` which
                // performs the replacement (custom_build.rs:925-928).
                let root_output_path = profile_dir.join(&unit_plan.root_output_file()?);
                fs::write(
                    &root_output_path,
                    out_dir_absolute.as_os_str().as_encoded_bytes(),
                )
                .await?;
                fs::set_mtime(&root_output_path, mtime).await?;
            }
            _ => bail!("unit type mismatch"),
        }

        // Mark the unit as restored. It's not _technically_ restored yet, but
        // this function will return an error later on when the workers join if
        // the restore doesn't succeed.
        debug!(?unit, "marking unit as restored after restoring");
        restored.units.insert(unit_hash.clone());
    }

    debug!("start sending files to restore workers");
    for file in files_to_restore {
        tx.send_async(file).await?;
    }
    drop(tx);
    debug!("done sending files to restore workers");

    debug!("start joining restore workers");
    while let Some(worker) = workers.join_next().await {
        worker
            .context("could not join worker")?
            .context("worker returned an error")?;
    }
    debug!("done joining restore workers");

    Ok(restored)
}

async fn restore_worker(
    rx: flume::Receiver<FileRestoreKey>,
    cas: CourierCas,
    progress: TransferBar,
    restored: Restored,
    restore_progress: RestoreProgress,
) -> Result<()> {
    const BATCH_SIZE: usize = 50;
    let mut batch = Vec::new();
    while let Ok(file) = rx.recv_async().await {
        debug!(?file, "worker got file");

        // Add the file to the batch.
        batch.push(file);
        debug!(len = ?batch.len(), "batch length");
        // If the batch is not full, wait for the batch to fill.
        if batch.len() < BATCH_SIZE {
            debug!("batch not full, waiting for more files");
            continue;
        }
        debug!("batch full, restoring");

        // Restore batches once full.
        let batch_to_restore = std::mem::take(&mut batch);
        restore_batch(
            batch_to_restore,
            &cas,
            &progress,
            &restored,
            &restore_progress,
        )
        .await?;
    }
    debug!("worker rx closed");

    // Once the channel closes, there may still be a partially filled batch
    // remaining. Restore the remaining files in the batch.
    if !batch.is_empty() {
        debug!(?batch, "restoring remaining batch");
        restore_batch(batch, &cas, &progress, &restored, &restore_progress).await?;
        debug!("done restoring remaining batch");
    }

    Ok(())
}

#[instrument(skip_all)]
async fn restore_batch(
    batch: Vec<FileRestoreKey>,
    cas: &CourierCas,
    progress: &TransferBar,
    restored: &Restored,
    restore_progress: &RestoreProgress,
) -> Result<()> {
    debug!(?batch, "restoring batch");

    // Note that you can have multiple files with the same key.
    // Build a map of object keys to file restore keys.
    let mut key_to_files = HashMap::new();
    for file in batch {
        key_to_files
            .entry(file.key.clone())
            .or_insert(vec![])
            .push(file);
    }

    // Now that keys are deduplicated, we can send them to the CAS; this way we
    // avoid making the server send multiple copies of the same file content.
    let keys = key_to_files.keys().cloned().collect::<Vec<_>>();

    // For each streamed CAS key, restore the file to the local filesystem.
    debug!(?keys, "start fetching files from CAS");
    let mut res = cas.get_bulk(keys).await?;
    debug!("start streaming response from CAS");
    while let Some(result) = res.next().await {
        match result {
            Ok((key, data)) => {
                debug!(?key, "CAS stream entry");
                let files = key_to_files
                    .remove(&key)
                    .ok_or_eyre("unrecognized key from CAS bulk response")?;
                for file in files {
                    restored.files.insert(file.key);

                    progress.add_files(1);
                    progress.add_bytes(data.len() as u64);

                    // Call the write callback to handle all file operations.
                    debug!(?key, "calling write callback");
                    (file.write)(&data).await?;
                    debug!(?key, "done calling write callback");

                    // Remove the key from the unit's pending keys.
                    let pending_keys = restore_progress
                        .units
                        .get_mut(&file.unit_hash)
                        .ok_or_eyre("unit hash restore progress not initialized")?;
                    // We ignore whether the key is actually present, because
                    // keys might be double-removed if they are present multiple
                    // times in the same unit, which can occur if a unit has two
                    // files that have the same contents (e.g. are both empty).
                    pending_keys.remove(&key);
                    if pending_keys.is_empty() {
                        debug!(?file.unit_hash, "unit has been fully restored");
                        progress.inc(1);
                    }
                }
            }
            Err(error) => {
                warn!(?error, "failed to fetch file from CAS");
            }
        }
    }
    debug!("done streaming response from CAS");

    Ok(())
}

fn unit_type_name(unit: &UnitPlan) -> &'static str {
    match unit {
        UnitPlan::LibraryCrate(_) => "LibraryCrate",
        UnitPlan::BuildScriptCompilation(_) => "BuildScriptCompilation",
        UnitPlan::BuildScriptExecution(_) => "BuildScriptExecution",
    }
}

/// Filter units to only those with complete dependency chains.
///
/// This function is also used by local restore to determine which units can be restored.
///
/// When the server returns some units but not their dependencies (e.g., due to
/// glibc incompatibility), restoring those units will fail during fingerprint
/// rewriting because the dependency's fingerprint hash mapping is never
/// recorded.
///
/// More importantly, Cargo's staleness check propagates transitively[^1]: if
/// any transitive dependency is stale, Cargo marks all dependents as stale too
/// and rebuilds them. So there's no benefit to restoring units with incomplete
/// deps.
///
/// This function filters out units whose dependencies are not available in
/// either:
/// - The cache response (`saved_units`)
/// - Already present on disk (`units_to_skip`)
///
/// The `units` slice must be in dependency order (dependencies before
/// dependents). This is guaranteed by `Workspace::units()` which builds units
/// from Cargo's build plan, which is topologically sorted because `plan.add`
/// is called after recursively compiling all dependencies[^2].
///
/// Because units are in dependency order, filtering a unit cascades to its
/// dependents automatically.
///
/// [^1]: https://github.com/rust-lang/cargo/blob/f2729c026922c086a4eaac29d23864fb4faeb71b/src/cargo/core/compiler/fingerprint/mod.rs#L1240-L1247
/// [^2]: https://github.com/rust-lang/cargo/blob/0436f86288a4d9bce1c712c4eea5b05eb82682b9/src/cargo/core/compiler/mod.rs#L238-L241
///
/// Returns:
/// - A set of unit hashes that should be skipped (filtered out)
/// - The count of units filtered due to incomplete dependencies
pub fn filter_units_with_incomplete_deps(
    units: &[UnitPlan],
    saved_units: &CargoRestoreResponse,
    units_to_skip: &HashSet<UnitHash>,
) -> (HashSet<UnitHash>, usize) {
    let mut available = saved_units
        .iter()
        .map(|(k, _)| UnitHash::from(k.as_str()))
        .chain(units_to_skip.iter().cloned())
        .collect::<HashSet<_>>();

    let mut filtered_hashes = HashSet::new();
    let mut filtered_count = 0;

    for unit in units {
        let unit_hash = &unit.info().unit_hash;

        if !available.contains(unit_hash) {
            continue;
        }

        let missing_dep = unit
            .info()
            .deps
            .iter()
            .find(|dep_hash| !available.contains(dep_hash));

        if let Some(missing_dep_hash) = missing_dep {
            available.remove(unit_hash);
            filtered_hashes.insert(unit_hash.clone());
            filtered_count += 1;

            debug!(
                %unit_hash,
                package = %unit.info().package_name,
                %missing_dep_hash,
                "filtering unit: incomplete dependency chain"
            );
        }
    }

    (filtered_hashes, filtered_count)
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::cargo::{LibraryCrateUnitPlan, RustcTarget, UnitPlanInfo};
    use crate::path::AbsFilePath;
    use clients::courier::v1::{
        Fingerprint as SavedFingerprint, Key, LibraryCrateUnitPlan as SavedLibraryCratePlan,
        LibraryFiles, SavedUnit, UnitPlanInfo as SavedUnitPlanInfo,
    };
    use pretty_assertions::assert_eq as pretty_assert_eq;

    fn make_unit_plan(hash: &str, package: &str, deps: Vec<&str>) -> UnitPlan {
        UnitPlan::LibraryCrate(LibraryCrateUnitPlan {
            info: UnitPlanInfo {
                unit_hash: hash.into(),
                package_name: String::from(package),
                package_version: String::from("1.0.0"),
                crate_name: String::from(package),
                target_arch: RustcTarget::ImplicitHost,
                deps: deps.into_iter().map(UnitHash::from).collect(),
            },
            src_path: AbsFilePath::try_from("/test/src/lib.rs").unwrap(),
            outputs: vec![],
        })
    }

    fn test_key(content: &[u8]) -> Key {
        Key::from_buffer(content)
    }

    fn make_saved_unit(hash: &str) -> SavedUnit {
        let info = SavedUnitPlanInfo::builder()
            .unit_hash(hash)
            .package_name("pkg")
            .crate_name("pkg")
            .maybe_target_arch(Some("x86_64-unknown-linux-gnu"))
            .build();

        let files = LibraryFiles::builder()
            .output_files(vec![])
            .fingerprint(SavedFingerprint::from(String::from("test-fingerprint")))
            .dep_info_file(test_key(b"dep-info"))
            .encoded_dep_info_file(test_key(b"encoded-dep-info"))
            .build();

        let plan = SavedLibraryCratePlan::builder()
            .info(info)
            .src_path("test.rs")
            .outputs(vec![] as Vec<clients::courier::v1::DiskPath>)
            .build();

        SavedUnit::LibraryCrate(files, plan)
    }

    #[test]
    fn empty_units() {
        let units = vec![];
        let saved = CargoRestoreResponse::default();
        let skip = HashSet::new();

        let (filtered, count) = filter_units_with_incomplete_deps(&units, &saved, &skip);

        pretty_assert_eq!(filtered, HashSet::new());
        pretty_assert_eq!(count, 0);
    }

    #[test]
    fn unit_with_no_deps_passes() {
        // Unit A has no deps and is in cache -> should pass
        let units = vec![make_unit_plan("A", "pkg-a", vec![])];
        let saved = CargoRestoreResponse::new([("A", make_saved_unit("A"))]);
        let skip = HashSet::new();

        let (filtered, count) = filter_units_with_incomplete_deps(&units, &saved, &skip);

        pretty_assert_eq!(filtered, HashSet::new());
        pretty_assert_eq!(count, 0);
    }

    #[test]
    fn unit_not_in_cache_not_counted() {
        // Unit A not in cache -> not counted in filtered (will be rebuilt anyway)
        let units = vec![make_unit_plan("A", "pkg-a", vec![])];
        let saved = CargoRestoreResponse::default();
        let skip = HashSet::new();

        let (filtered, count) = filter_units_with_incomplete_deps(&units, &saved, &skip);

        pretty_assert_eq!(filtered, HashSet::new());
        pretty_assert_eq!(count, 0);
    }

    #[test]
    fn unit_with_dep_on_disk_passes() {
        // A depends on B; B is on disk (skipped), A is in cache -> should pass
        let units = vec![
            make_unit_plan("B", "pkg-b", vec![]),
            make_unit_plan("A", "pkg-a", vec!["B"]),
        ];
        let saved = CargoRestoreResponse::new([("A", make_saved_unit("A"))]);
        let skip = HashSet::from([UnitHash::from("B")]);

        let (filtered, count) = filter_units_with_incomplete_deps(&units, &saved, &skip);

        pretty_assert_eq!(filtered, HashSet::new());
        pretty_assert_eq!(count, 0);
    }

    #[test]
    fn unit_with_dep_in_cache_passes() {
        // Unit A depends on B; both in cache -> A should pass
        let units = vec![
            make_unit_plan("B", "pkg-b", vec![]),
            make_unit_plan("A", "pkg-a", vec!["B"]),
        ];
        let saved =
            CargoRestoreResponse::new([("B", make_saved_unit("B")), ("A", make_saved_unit("A"))]);
        let skip = HashSet::new();

        let (filtered, count) = filter_units_with_incomplete_deps(&units, &saved, &skip);

        pretty_assert_eq!(filtered, HashSet::new());
        pretty_assert_eq!(count, 0);
    }

    #[test]
    fn unit_with_missing_dep_filtered() {
        // A depends on B; A is in cache but B is not -> A should be filtered
        let units = vec![
            make_unit_plan("B", "pkg-b", vec![]),
            make_unit_plan("A", "pkg-a", vec!["B"]),
        ];
        let saved = CargoRestoreResponse::new([("A", make_saved_unit("A"))]);
        let skip = HashSet::new();

        let (filtered, count) = filter_units_with_incomplete_deps(&units, &saved, &skip);

        pretty_assert_eq!(filtered, HashSet::from([UnitHash::from("A")]));
        pretty_assert_eq!(count, 1);
    }

    #[test]
    fn cascading_filter() {
        // A -> B -> C where C is missing; both A and B should be filtered
        let units = vec![
            make_unit_plan("C", "pkg-c", vec![]),
            make_unit_plan("B", "pkg-b", vec!["C"]),
            make_unit_plan("A", "pkg-a", vec!["B"]),
        ];
        let saved =
            CargoRestoreResponse::new([("A", make_saved_unit("A")), ("B", make_saved_unit("B"))]);
        let skip = HashSet::new();

        let (filtered, count) = filter_units_with_incomplete_deps(&units, &saved, &skip);

        pretty_assert_eq!(
            filtered,
            HashSet::from([UnitHash::from("B"), UnitHash::from("A")])
        );
        pretty_assert_eq!(count, 2);
    }

    #[test]
    fn partial_graph_with_some_deps_available() {
        // A -> B (B available), C -> D (D missing), E -> C (C filtered, so E filtered
        // too)
        let units = vec![
            make_unit_plan("B", "pkg-b", vec![]),
            make_unit_plan("D", "pkg-d", vec![]),
            make_unit_plan("A", "pkg-a", vec!["B"]),
            make_unit_plan("C", "pkg-c", vec!["D"]),
            make_unit_plan("E", "pkg-e", vec!["C"]),
        ];
        let saved = CargoRestoreResponse::new([
            ("A", make_saved_unit("A")),
            ("B", make_saved_unit("B")),
            ("C", make_saved_unit("C")),
            ("E", make_saved_unit("E")),
        ]);
        let skip = HashSet::new();

        let (filtered, count) = filter_units_with_incomplete_deps(&units, &saved, &skip);

        pretty_assert_eq!(
            filtered,
            HashSet::from([UnitHash::from("C"), UnitHash::from("E")])
        );
        pretty_assert_eq!(count, 2);
    }
}
